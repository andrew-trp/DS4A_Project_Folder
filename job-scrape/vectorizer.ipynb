{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "06a18a7efdeadffcc5351ba6b0cdd9096ebc6d9838a2f83815b27dd8f3831157"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.1.3-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.3\n",
      "    Uninstalling pip-20.2.3:\n",
      "      Successfully uninstalled pip-20.2.3\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\AndrewLinebarger\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-0m20havm\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp39-cp39-win_amd64.whl (6.9 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\andrewlinebarger\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\andrewlinebarger\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andrewlinebarger\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "import regex\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['Apple','Pear','Banana']\n",
    "for word in words:\n",
    "    word = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Apple', 'Pear', 'Banana']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                             titles  \\\n",
       "0           0                 FULLY REMOTE - Data Scientist\\nnew   \n",
       "1           1                     Associate - Risk Modeling\\nnew   \n",
       "2           2  Data Scientist Director - Focused Analytics So...   \n",
       "\n",
       "                   companies  \\\n",
       "0                CyberCoders   \n",
       "1  JPMorgan Chase Bank, N.A.   \n",
       "2  JPMorgan Chase Bank, N.A.   \n",
       "\n",
       "                                               links  date_listed  \\\n",
       "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...  Just posted   \n",
       "1  https://www.indeed.com/rc/clk?jk=953c414a32b6f...  Just posted   \n",
       "2  https://www.indeed.com/rc/clk?jk=54d3b407aa701...  Just posted   \n",
       "\n",
       "                                         description  \n",
       "0  FULLY REMOTE - Data Scientist\\nIf you are a Da...  \n",
       "1  JPMorgan Chase & Co. (NYSE: JPM) is a leading ...  \n",
       "2  ED, Data Scientist Director\\nCCB FAST Data Sci...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>titles</th>\n      <th>companies</th>\n      <th>links</th>\n      <th>date_listed</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>FULLY REMOTE - Data Scientist\\nnew</td>\n      <td>CyberCoders</td>\n      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n      <td>Just posted</td>\n      <td>FULLY REMOTE - Data Scientist\\nIf you are a Da...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Associate - Risk Modeling\\nnew</td>\n      <td>JPMorgan Chase Bank, N.A.</td>\n      <td>https://www.indeed.com/rc/clk?jk=953c414a32b6f...</td>\n      <td>Just posted</td>\n      <td>JPMorgan Chase &amp; Co. (NYSE: JPM) is a leading ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Data Scientist Director - Focused Analytics So...</td>\n      <td>JPMorgan Chase Bank, N.A.</td>\n      <td>https://www.indeed.com/rc/clk?jk=54d3b407aa701...</td>\n      <td>Just posted</td>\n      <td>ED, Data Scientist Director\\nCCB FAST Data Sci...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "rdf = pd.read_excel('results.xls')\n",
    "rdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FULLY REMOTE - Data Scientist\nIf you are a Data Scientist with Python and/or Scala experience, please read on! We're based in the New York City area and are a proven and growing commercial real estate technology company. We're innovators responsible for creating state of the art technology that helps make the commercial real estate industry more efficient and highly advanced. Our culture is everything to us, we're a small group of like minded people who share the same passion of providing the highest quality product. We are looking for people to join who have a passion for technology, working collaboratively and is looking to be a part of a company that is changing the industry through innovation and having fun while we do it! This Data Scientist will play a critical role in creating models that are embedded in our data pipelines and will support our entire platform! You will work with a next level data infrastructure that supports and links datasets at scale, while building a brand new product within our company. You will be leveraging machine learning and AI to better serve our customers to enhance the way to run their business. In addition, you'll collaborate with a stellar engineering team to ensure our support process is more efficient, get projects done in a timely manner and work as a team to be an integral part in the success of the company.\nTop Reasons to Work with UsGreat Compensation/BenefitsSteady room for growthWork with high end, innovative technology\nAwesome family like culture\nWhat You Will Be DoingImprove data quality and work to improve customer access to data\nDo you part in analytical work for feature generation\nModel choice and implementation within the platformWork to insure efficiency in the product by writing quality code and participating in testing and reviewsWrite process and metrics that will help to measure and improve the performance\nWork with engineering team to design, build and maintain ML models\nWhat You Need for this Position\nAt least  years of experience working with: Python and/or ScalaDemonstrate strong experience with feature development and random forests Nice to have: Distributed computing with Spark and/or PysparkWorking knowledge of SQLRedshiftHadoop\nSo, if you are a Data Scientist with Python and/or Scala experience, please apply today!Applicants must be authorized to work in the U.S.\nCyberCoders, Inc is proud to be an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law. Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdf[\"description\"] = rdf[\"description\"].replace(r'\\s\\s\\s',' ', regex=True)\n",
    "rdf[\"description\"] = rdf[\"description\"].replace(r'\\s\\s',' ', regex=True)\n",
    "rdf[\"description\"] = rdf[\"description\"].replace(r'[0-9]','', regex=True)\n",
    "# rdf[\"description\"] = rdf[\"description\"].replace(r'[^\\w\\s]+', '')\n",
    "print(rdf[\"description\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords1 = stopwords.words('english')\n",
    "\n",
    "new_clean_desc = []\n",
    "desc_list = rdf[\"description\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for desc in desc_list:\n",
    "    desc = desc.lower()\n",
    "    descwords = desc.split()\n",
    "    resultwords = [word for word in descwords if word.lower() not in stopwords.words('english')]\n",
    "    # This line was added in order to lemmatize the words\n",
    "    resultwords = [lemmatizer.lemmatize(word) for word in descwords]\n",
    "\n",
    "    result = ' '.join(resultwords)\n",
    "    \n",
    "    new_clean_desc.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'fully remote  data scientist if you are a data scientist with python andor scala experience please read on were based in the new york city area and are a proven and growing commercial real estate technology company were innovator responsible for creating state of the art technology that help make the commercial real estate industry more efficient and highly advanced our culture is everything to us were a small group of like minded people who share the same passion of providing the highest quality product we are looking for people to join who have a passion for technology working collaboratively and is looking to be a part of a company that is changing the industry through innovation and having fun while we do it this data scientist will play a critical role in creating model that are embedded in our data pipeline and will support our entire platform you will work with a next level data infrastructure that support and link datasets at scale while building a brand new product within our company you will be leveraging machine learning and ai to better serve our customer to enhance the way to run their business in addition youll collaborate with a stellar engineering team to ensure our support process is more efficient get project done in a timely manner and work a a team to be an integral part in the success of the company top reason to work with usgreat compensationbenefitssteady room for growthwork with high end innovative technology awesome family like culture what you will be doingimprove data quality and work to improve customer access to data do you part in analytical work for feature generation model choice and implementation within the platformwork to insure efficiency in the product by writing quality code and participating in testing and reviewswrite process and metric that will help to measure and improve the performance work with engineering team to design build and maintain ml model what you need for this position at least year of experience working with python andor scalademonstrate strong experience with feature development and random forest nice to have distributed computing with spark andor pysparkworking knowledge of sqlredshifthadoop so if you are a data scientist with python andor scala experience please apply todayapplicants must be authorized to work in the us cybercoders inc is proud to be an equal opportunity employer all qualified applicant will receive consideration for employment without regard to race color religion sex national origin disability protected veteran status or any other characteristic protected by law your right to work  in compliance with federal law all person hired will be required to verify identity and eligibility to work in the united state and to complete the required employment eligibility verification document form upon hire'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "rdf['clean_lc'] = new_clean_desc\n",
    "rdf['clean_lc'] = rdf[\"clean_lc\"].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "rdf['clean_lc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "descriptions = rdf['clean_lc']\n",
    "\n",
    "vectors = vectorizer.fit_transform(descriptions)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns\n",
    "# df.shape\n",
    "masculine_words = [\"active\",\"Adventurous\",\"Aggress*\",\"Ambitio*\",\"Ambition\",\"Analy*\",\"Assert*\",\"assertive\",\"Athlet*\",\"Autonom*\",\"autonomous\",\"Boast*\",\"Challeng*\",\"Compet*\",\"competition\",\"competitive\",\"compliance\",\"Confident\",\"control\",\"Courag*\",\"Decide\",\"Decision*\",\"Decisive\",\"Determin*\",\"Direct\",\"Domina*\",\"Dominant\",\"driven\",\"ensure\",\"Force*\",\"Greedy\",\"Headstrong\",\"Hierarch*\",\"hierarchical\",\"Hostil*\",\"Implusive\",\"Independen*\",\"Individual*\",\"Intellect*\",\"Lead*\",\"leading\",\"Logic\",\"manage\",\"Masculine\",\"must\",\"Objective\",\"Opinion\",\"Outspoken\",\"perform individually\",\"Persist\",\"ping pong/pool table\",\"Principle*\",\"progress\",\"Reckless\",\"rigid\",\"satisfy\",\"Self-confiden*\",\"Self-relian*\",\"Self-sufficien*\",\"Silicon Valley\",\"stand\",\"stock options\",\"Strong\",\"Stubborn\",\"Superior\",\"takes risk\",\"workforce\",\"seasoned\"]\n",
    "\n",
    "masculine_words2 = [\"active\",\"Adventurous\",\"Aggressive\", 'Aggression',\"Ambitious\",\"Ambition\",\"analytical\",\"Assertion\",\"assertive\",\"Athletic\", 'Athleticism',\"Autonomously\", 'Autonomy',\"autonomous\",\"Boast\", \"Boastful\",\"Challenge\", 'Challenging', \"competition\",\"competitive\",\"compliance\",\"Confident\",\"control\",\"Courage\", 'Courageous',\"Decide\",\"Decision\",\"Decisive\",\"Determined\", 'Determination',\"Direct\",'Dominance',\"Dominant\",\"driven\",\"ensure\",\"Force\", 'Forceful', \"Greedy\",\"Headstrong\",\"Hierarchy\",\"hierarchical\",\"Hostile\", 'Hostility',\"Implusive\",\"Independent\", 'Independence',\"Individual*\",\"Intellect\", 'Intellectual',\"Leader\", 'Lead',\"leading\",\"Logic\",\"manage\",\"Masculine\",\"must\",\"Objective\",\"Opinion\", 'opinionated',\"Outspoken\",\"perform individually\",\"Persist\", 'Persistent',\"ping pong/pool table\",\"Principle\", 'Principled',\"progress\",\"Reckless\",\"rigid\",\"satisfy\",\"Self-confidence\", 'Self-confident',\"Self-reliant\", 'Self-reliance',\"Self-sufficient\", 'Self-sufficience', 'Self', \"Silicon Valley\",\"stand\",\"stock options\",\"Strong\",\"Stubborn\",\"Superior\",\"takes\", \"risk\", 'reliant', 'reliance', 'sufficient', 'sufficience', \"workforce\",\"seasoned\"]\n",
    "\n",
    "masculine_words = [word.lower() for word in masculine_words]\n",
    "masculine_words2 = [word.lower() for word in masculine_words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['competitive', 'compliance', 'direct', 'driven', 'ensure', 'leading',\n       'manage', 'must', 'progress', 'strong', 'workforce'],\n      dtype='object')\nIndex(['analytical', 'athletic', 'autonomy', 'challenge', 'challenging',\n       'competitive', 'compliance', 'decision', 'determination', 'direct',\n       'driven', 'ensure', 'force', 'independent', 'lead', 'leader', 'leading',\n       'manage', 'must', 'principle', 'progress', 'risk', 'strong',\n       'workforce'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.intersection(masculine_words))\n",
    "print(df.columns.intersection(masculine_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "len(masculine_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "91\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    analytical  athletic  autonomy  challenge  challenging  competitive  \\\n",
       "0     0.025632  0.000000  0.000000   0.000000     0.000000     0.000000   \n",
       "1     0.017822  0.000000  0.000000   0.026995     0.000000     0.000000   \n",
       "2     0.084689  0.000000  0.000000   0.025657     0.000000     0.000000   \n",
       "3     0.027362  0.000000  0.000000   0.000000     0.046444     0.000000   \n",
       "4     0.051450  0.000000  0.000000   0.000000     0.000000     0.000000   \n",
       "5     0.052667  0.000000  0.000000   0.000000     0.029799     0.000000   \n",
       "6     0.000000  0.000000  0.000000   0.000000     0.000000     0.000000   \n",
       "7     0.000000  0.000000  0.000000   0.024303     0.000000     0.000000   \n",
       "8     0.000000  0.000000  0.035948   0.000000     0.000000     0.000000   \n",
       "9     0.000000  0.000000  0.000000   0.000000     0.000000     0.025410   \n",
       "10    0.038790  0.000000  0.000000   0.000000     0.000000     0.029378   \n",
       "11    0.000000  0.065973  0.000000   0.000000     0.000000     0.000000   \n",
       "12    0.000000  0.000000  0.000000   0.000000     0.000000     0.000000   \n",
       "13    0.018032  0.000000  0.000000   0.000000     0.000000     0.000000   \n",
       "14    0.000000  0.000000  0.000000   0.000000     0.000000     0.032395   \n",
       "\n",
       "    compliance  decision  determination    direct  ...      lead    leader  \\\n",
       "0     0.050104  0.000000       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "1     0.000000  0.000000       0.000000  0.000000  ...  0.000000  0.048942   \n",
       "2     0.000000  0.000000       0.000000  0.025657  ...  0.000000  0.046515   \n",
       "3     0.000000  0.000000       0.000000  0.041447  ...  0.000000  0.000000   \n",
       "4     0.000000  0.000000       0.000000  0.000000  ...  0.077934  0.000000   \n",
       "5     0.000000  0.018868       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "6     0.000000  0.000000       0.000000  0.000000  ...  0.000000  0.034925   \n",
       "7     0.000000  0.017244       0.031362  0.000000  ...  0.072909  0.044061   \n",
       "8     0.000000  0.039530       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "9     0.000000  0.018029       0.000000  0.000000  ...  0.025410  0.000000   \n",
       "10    0.000000  0.083380       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "11    0.000000  0.000000       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "12    0.000000  0.013390       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "13    0.000000  0.019380       0.000000  0.000000  ...  0.000000  0.000000   \n",
       "14    0.000000  0.000000       0.000000  0.032395  ...  0.000000  0.000000   \n",
       "\n",
       "     leading    manage      must  principle  progress      risk    strong  \\\n",
       "0   0.000000  0.000000  0.032229   0.000000  0.000000  0.000000  0.022367   \n",
       "1   0.022408  0.000000  0.000000   0.000000  0.000000  0.188968  0.046654   \n",
       "2   0.063892  0.106487  0.000000   0.000000  0.000000  0.000000  0.014780   \n",
       "3   0.000000  0.034405  0.068809   0.000000  0.000000  0.000000  0.071630   \n",
       "4   0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.044896   \n",
       "5   0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.031981  0.031981  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000   0.000000  0.027233  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.011562   0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000   0.032791  0.028473  0.000000  0.029276   \n",
       "10  0.000000  0.000000  0.024387   0.000000  0.000000  0.000000  0.016924   \n",
       "11  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.029451   \n",
       "12  0.046994  0.015665  0.015665   0.000000  0.000000  0.000000  0.010871   \n",
       "13  0.022673  0.022673  0.000000   0.000000  0.000000  0.027314  0.015735   \n",
       "14  0.000000  0.000000  0.000000   0.000000  0.000000  0.194370  0.000000   \n",
       "\n",
       "    workforce  \n",
       "0    0.000000  \n",
       "1    0.026995  \n",
       "2    0.025657  \n",
       "3    0.000000  \n",
       "4    0.000000  \n",
       "5    0.000000  \n",
       "6    0.038528  \n",
       "7    0.000000  \n",
       "8    0.000000  \n",
       "9    0.000000  \n",
       "10   0.000000  \n",
       "11   0.000000  \n",
       "12   0.000000  \n",
       "13   0.000000  \n",
       "14   0.000000  \n",
       "\n",
       "[15 rows x 24 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>analytical</th>\n      <th>athletic</th>\n      <th>autonomy</th>\n      <th>challenge</th>\n      <th>challenging</th>\n      <th>competitive</th>\n      <th>compliance</th>\n      <th>decision</th>\n      <th>determination</th>\n      <th>direct</th>\n      <th>...</th>\n      <th>lead</th>\n      <th>leader</th>\n      <th>leading</th>\n      <th>manage</th>\n      <th>must</th>\n      <th>principle</th>\n      <th>progress</th>\n      <th>risk</th>\n      <th>strong</th>\n      <th>workforce</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.025632</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.050104</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032229</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.022367</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.017822</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.026995</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.048942</td>\n      <td>0.022408</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.188968</td>\n      <td>0.046654</td>\n      <td>0.026995</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.084689</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025657</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025657</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.046515</td>\n      <td>0.063892</td>\n      <td>0.106487</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014780</td>\n      <td>0.025657</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.027362</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.046444</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.041447</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.034405</td>\n      <td>0.068809</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.071630</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.051450</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.077934</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.044896</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.052667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.029799</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.018868</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.034925</td>\n      <td>0.031981</td>\n      <td>0.031981</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.038528</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.024303</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.017244</td>\n      <td>0.031362</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.072909</td>\n      <td>0.044061</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.027233</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.035948</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.039530</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.011562</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025410</td>\n      <td>0.000000</td>\n      <td>0.018029</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.025410</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032791</td>\n      <td>0.028473</td>\n      <td>0.000000</td>\n      <td>0.029276</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.038790</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.029378</td>\n      <td>0.000000</td>\n      <td>0.083380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.024387</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.016924</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000000</td>\n      <td>0.065973</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.029451</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013390</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.046994</td>\n      <td>0.015665</td>\n      <td>0.015665</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010871</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.018032</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.022673</td>\n      <td>0.022673</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.027314</td>\n      <td>0.015735</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032395</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.032395</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.194370</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "print(len(masculine_words2))\n",
    "df[df.columns.intersection(masculine_words2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rdf['clean_lc'][0]\n",
    "wtkn = nltk.word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in wtkn:\n",
    "    word = lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fully',\n",
       " 'remote',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'python',\n",
       " 'andor',\n",
       " 'scala',\n",
       " 'experience',\n",
       " 'please',\n",
       " 'read',\n",
       " 'on',\n",
       " 'were',\n",
       " 'based',\n",
       " 'new',\n",
       " 'york',\n",
       " 'city',\n",
       " 'area',\n",
       " 'proven',\n",
       " 'growing',\n",
       " 'commercial',\n",
       " 'real',\n",
       " 'estate',\n",
       " 'technology',\n",
       " 'company',\n",
       " 'were',\n",
       " 'innovators',\n",
       " 'responsible',\n",
       " 'creating',\n",
       " 'state',\n",
       " 'art',\n",
       " 'technology',\n",
       " 'helps',\n",
       " 'make',\n",
       " 'commercial',\n",
       " 'real',\n",
       " 'estate',\n",
       " 'industry',\n",
       " 'efficient',\n",
       " 'highly',\n",
       " 'advanced',\n",
       " 'culture',\n",
       " 'everything',\n",
       " 'us',\n",
       " 'were',\n",
       " 'small',\n",
       " 'group',\n",
       " 'like',\n",
       " 'minded',\n",
       " 'people',\n",
       " 'share',\n",
       " 'passion',\n",
       " 'providing',\n",
       " 'highest',\n",
       " 'quality',\n",
       " 'product',\n",
       " 'looking',\n",
       " 'people',\n",
       " 'join',\n",
       " 'passion',\n",
       " 'technology',\n",
       " 'working',\n",
       " 'collaboratively',\n",
       " 'looking',\n",
       " 'part',\n",
       " 'company',\n",
       " 'changing',\n",
       " 'industry',\n",
       " 'innovation',\n",
       " 'fun',\n",
       " 'it',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'play',\n",
       " 'critical',\n",
       " 'role',\n",
       " 'creating',\n",
       " 'models',\n",
       " 'embedded',\n",
       " 'data',\n",
       " 'pipelines',\n",
       " 'support',\n",
       " 'entire',\n",
       " 'platform',\n",
       " 'work',\n",
       " 'next',\n",
       " 'level',\n",
       " 'data',\n",
       " 'infrastructure',\n",
       " 'supports',\n",
       " 'links',\n",
       " 'datasets',\n",
       " 'scale',\n",
       " 'building',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'product',\n",
       " 'within',\n",
       " 'company',\n",
       " 'leveraging',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'ai',\n",
       " 'better',\n",
       " 'serve',\n",
       " 'customers',\n",
       " 'enhance',\n",
       " 'way',\n",
       " 'run',\n",
       " 'business',\n",
       " 'addition',\n",
       " 'collaborate',\n",
       " 'stellar',\n",
       " 'engineering',\n",
       " 'team',\n",
       " 'ensure',\n",
       " 'support',\n",
       " 'process',\n",
       " 'efficient',\n",
       " 'get',\n",
       " 'projects',\n",
       " 'done',\n",
       " 'timely',\n",
       " 'manner',\n",
       " 'work',\n",
       " 'team',\n",
       " 'integral',\n",
       " 'part',\n",
       " 'success',\n",
       " 'company',\n",
       " 'top',\n",
       " 'reasons',\n",
       " 'work',\n",
       " 'usgreat',\n",
       " 'compensationbenefitssteady',\n",
       " 'room',\n",
       " 'growthwork',\n",
       " 'high',\n",
       " 'end',\n",
       " 'innovative',\n",
       " 'technology',\n",
       " 'awesome',\n",
       " 'family',\n",
       " 'like',\n",
       " 'culture',\n",
       " 'doingimprove',\n",
       " 'data',\n",
       " 'quality',\n",
       " 'work',\n",
       " 'improve',\n",
       " 'customer',\n",
       " 'access',\n",
       " 'data',\n",
       " 'part',\n",
       " 'analytical',\n",
       " 'work',\n",
       " 'feature',\n",
       " 'generation',\n",
       " 'model',\n",
       " 'choice',\n",
       " 'implementation',\n",
       " 'within',\n",
       " 'platformwork',\n",
       " 'insure',\n",
       " 'efficiency',\n",
       " 'product',\n",
       " 'writing',\n",
       " 'quality',\n",
       " 'code',\n",
       " 'participating',\n",
       " 'testing',\n",
       " 'reviewswrite',\n",
       " 'process',\n",
       " 'metrics',\n",
       " 'help',\n",
       " 'measure',\n",
       " 'improve',\n",
       " 'performance',\n",
       " 'work',\n",
       " 'engineering',\n",
       " 'team',\n",
       " 'design',\n",
       " 'build',\n",
       " 'maintain',\n",
       " 'ml',\n",
       " 'models',\n",
       " 'need',\n",
       " 'position',\n",
       " 'least',\n",
       " 'years',\n",
       " 'experience',\n",
       " 'working',\n",
       " 'with',\n",
       " 'python',\n",
       " 'andor',\n",
       " 'scalademonstrate',\n",
       " 'strong',\n",
       " 'experience',\n",
       " 'feature',\n",
       " 'development',\n",
       " 'random',\n",
       " 'forests',\n",
       " 'nice',\n",
       " 'have',\n",
       " 'distributed',\n",
       " 'computing',\n",
       " 'spark',\n",
       " 'andor',\n",
       " 'pysparkworking',\n",
       " 'knowledge',\n",
       " 'sqlredshifthadoop',\n",
       " 'so',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'python',\n",
       " 'andor',\n",
       " 'scala',\n",
       " 'experience',\n",
       " 'please',\n",
       " 'apply',\n",
       " 'todayapplicants',\n",
       " 'must',\n",
       " 'authorized',\n",
       " 'work',\n",
       " 'us',\n",
       " 'cybercoders',\n",
       " 'inc',\n",
       " 'proud',\n",
       " 'equal',\n",
       " 'opportunity',\n",
       " 'employer',\n",
       " 'qualified',\n",
       " 'applicants',\n",
       " 'receive',\n",
       " 'consideration',\n",
       " 'employment',\n",
       " 'without',\n",
       " 'regard',\n",
       " 'race',\n",
       " 'color',\n",
       " 'religion',\n",
       " 'sex',\n",
       " 'national',\n",
       " 'origin',\n",
       " 'disability',\n",
       " 'protected',\n",
       " 'veteran',\n",
       " 'status',\n",
       " 'characteristic',\n",
       " 'protected',\n",
       " 'law',\n",
       " 'right',\n",
       " 'work',\n",
       " 'compliance',\n",
       " 'federal',\n",
       " 'law',\n",
       " 'persons',\n",
       " 'hired',\n",
       " 'required',\n",
       " 'verify',\n",
       " 'identity',\n",
       " 'eligibility',\n",
       " 'work',\n",
       " 'united',\n",
       " 'states',\n",
       " 'complete',\n",
       " 'required',\n",
       " 'employment',\n",
       " 'eligibility',\n",
       " 'verification',\n",
       " 'document',\n",
       " 'form',\n",
       " 'upon',\n",
       " 'hire']"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "wtkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to clean up punctuation before we run through the vectorizer\n",
    "# we will need to remove stopwords as well\n",
    "rgx_split = regex.split(\"[\\s\\.\\,]\",text)\n",
    "rgx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_stemm = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fulli',\n",
       " 'remot',\n",
       " '-',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'with',\n",
       " 'python',\n",
       " 'and/or',\n",
       " 'scala',\n",
       " 'experi',\n",
       " ',',\n",
       " 'pleas',\n",
       " 'read',\n",
       " 'on',\n",
       " '!',\n",
       " 'we',\n",
       " 're',\n",
       " 'base',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'york',\n",
       " 'citi',\n",
       " 'area',\n",
       " 'and',\n",
       " 'are',\n",
       " 'a',\n",
       " 'proven',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'commerci',\n",
       " 'real',\n",
       " 'estat',\n",
       " 'technolog',\n",
       " 'compani',\n",
       " '.',\n",
       " 'we',\n",
       " 're',\n",
       " 'innov',\n",
       " 'respons',\n",
       " 'for',\n",
       " 'creat',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'technolog',\n",
       " 'that',\n",
       " 'help',\n",
       " 'make',\n",
       " 'the',\n",
       " 'commerci',\n",
       " 'real',\n",
       " 'estat',\n",
       " 'industri',\n",
       " 'more',\n",
       " 'effici',\n",
       " 'and',\n",
       " 'high',\n",
       " 'advanc',\n",
       " '.',\n",
       " 'our',\n",
       " 'cultur',\n",
       " 'is',\n",
       " 'everyth',\n",
       " 'to',\n",
       " 'us',\n",
       " ',',\n",
       " 'we',\n",
       " 're',\n",
       " 'a',\n",
       " 'small',\n",
       " 'group',\n",
       " 'of',\n",
       " 'like',\n",
       " 'mind',\n",
       " 'peopl',\n",
       " 'who',\n",
       " 'share',\n",
       " 'the',\n",
       " 'same',\n",
       " 'passion',\n",
       " 'of',\n",
       " 'provid',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'qualiti',\n",
       " 'product',\n",
       " '.',\n",
       " 'we',\n",
       " 'are',\n",
       " 'look',\n",
       " 'for',\n",
       " 'peopl',\n",
       " 'to',\n",
       " 'join',\n",
       " 'who',\n",
       " 'have',\n",
       " 'a',\n",
       " 'passion',\n",
       " 'for',\n",
       " 'technolog',\n",
       " ',',\n",
       " 'work',\n",
       " 'collabor',\n",
       " 'and',\n",
       " 'is',\n",
       " 'look',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'compani',\n",
       " 'that',\n",
       " 'is',\n",
       " 'chang',\n",
       " 'the',\n",
       " 'industri',\n",
       " 'through',\n",
       " 'innov',\n",
       " 'and',\n",
       " 'have',\n",
       " 'fun',\n",
       " 'while',\n",
       " 'we',\n",
       " 'do',\n",
       " 'it',\n",
       " '!',\n",
       " 'this',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'will',\n",
       " 'play',\n",
       " 'a',\n",
       " 'critic',\n",
       " 'role',\n",
       " 'in',\n",
       " 'creat',\n",
       " 'model',\n",
       " 'that',\n",
       " 'are',\n",
       " 'embed',\n",
       " 'in',\n",
       " 'our',\n",
       " 'data',\n",
       " 'pipelin',\n",
       " 'and',\n",
       " 'will',\n",
       " 'support',\n",
       " 'our',\n",
       " 'entir',\n",
       " 'platform',\n",
       " '!',\n",
       " 'you',\n",
       " 'will',\n",
       " 'work',\n",
       " 'with',\n",
       " 'a',\n",
       " 'next',\n",
       " 'level',\n",
       " 'data',\n",
       " 'infrastructur',\n",
       " 'that',\n",
       " 'support',\n",
       " 'and',\n",
       " 'link',\n",
       " 'dataset',\n",
       " 'at',\n",
       " 'scale',\n",
       " ',',\n",
       " 'while',\n",
       " 'build',\n",
       " 'a',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'product',\n",
       " 'within',\n",
       " 'our',\n",
       " 'compani',\n",
       " '.',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'leverag',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'ai',\n",
       " 'to',\n",
       " 'better',\n",
       " 'serv',\n",
       " 'our',\n",
       " 'custom',\n",
       " 'to',\n",
       " 'enhanc',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'run',\n",
       " 'their',\n",
       " 'busi',\n",
       " '.',\n",
       " 'in',\n",
       " 'addit',\n",
       " ',',\n",
       " 'you',\n",
       " 'll',\n",
       " 'collabor',\n",
       " 'with',\n",
       " 'a',\n",
       " 'stellar',\n",
       " 'engin',\n",
       " 'team',\n",
       " 'to',\n",
       " 'ensur',\n",
       " 'our',\n",
       " 'support',\n",
       " 'process',\n",
       " 'is',\n",
       " 'more',\n",
       " 'effici',\n",
       " ',',\n",
       " 'get',\n",
       " 'project',\n",
       " 'done',\n",
       " 'in',\n",
       " 'a',\n",
       " 'time',\n",
       " 'manner',\n",
       " 'and',\n",
       " 'work',\n",
       " 'as',\n",
       " 'a',\n",
       " 'team',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'integr',\n",
       " 'part',\n",
       " 'in',\n",
       " 'the',\n",
       " 'success',\n",
       " 'of',\n",
       " 'the',\n",
       " 'compani',\n",
       " '.',\n",
       " 'top',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'usgreat',\n",
       " 'compensation/benefitssteadi',\n",
       " 'room',\n",
       " 'for',\n",
       " 'growthwork',\n",
       " 'with',\n",
       " 'high',\n",
       " 'end',\n",
       " ',',\n",
       " 'innov',\n",
       " 'technolog',\n",
       " 'awesom',\n",
       " 'famili',\n",
       " 'like',\n",
       " 'cultur',\n",
       " 'what',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'doingimprov',\n",
       " 'data',\n",
       " 'qualiti',\n",
       " 'and',\n",
       " 'work',\n",
       " 'to',\n",
       " 'improv',\n",
       " 'custom',\n",
       " 'access',\n",
       " 'to',\n",
       " 'data',\n",
       " 'do',\n",
       " 'you',\n",
       " 'part',\n",
       " 'in',\n",
       " 'analyt',\n",
       " 'work',\n",
       " 'for',\n",
       " 'featur',\n",
       " 'generat',\n",
       " 'model',\n",
       " 'choic',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'within',\n",
       " 'the',\n",
       " 'platformwork',\n",
       " 'to',\n",
       " 'insur',\n",
       " 'effici',\n",
       " 'in',\n",
       " 'the',\n",
       " 'product',\n",
       " 'by',\n",
       " 'write',\n",
       " 'qualiti',\n",
       " 'code',\n",
       " 'and',\n",
       " 'particip',\n",
       " 'in',\n",
       " 'test',\n",
       " 'and',\n",
       " 'reviewswrit',\n",
       " 'process',\n",
       " 'and',\n",
       " 'metric',\n",
       " 'that',\n",
       " 'will',\n",
       " 'help',\n",
       " 'to',\n",
       " 'measur',\n",
       " 'and',\n",
       " 'improv',\n",
       " 'the',\n",
       " 'perform',\n",
       " 'work',\n",
       " 'with',\n",
       " 'engin',\n",
       " 'team',\n",
       " 'to',\n",
       " 'design',\n",
       " ',',\n",
       " 'build',\n",
       " 'and',\n",
       " 'maintain',\n",
       " 'ml',\n",
       " 'model',\n",
       " 'what',\n",
       " 'you',\n",
       " 'need',\n",
       " 'for',\n",
       " 'this',\n",
       " 'posit',\n",
       " 'at',\n",
       " 'least',\n",
       " '2',\n",
       " 'year',\n",
       " 'of',\n",
       " 'experi',\n",
       " 'work',\n",
       " 'with',\n",
       " ':',\n",
       " 'python',\n",
       " 'and/or',\n",
       " 'scalademonstr',\n",
       " 'strong',\n",
       " 'experi',\n",
       " 'with',\n",
       " 'featur',\n",
       " 'develop',\n",
       " 'and',\n",
       " 'random',\n",
       " 'forest',\n",
       " 'nice',\n",
       " 'to',\n",
       " 'have',\n",
       " ':',\n",
       " 'distribut',\n",
       " 'comput',\n",
       " 'with',\n",
       " 'spark',\n",
       " 'and/or',\n",
       " 'pysparkwork',\n",
       " 'knowledg',\n",
       " 'of',\n",
       " 'sqlredshifthadoop',\n",
       " 'so',\n",
       " ',',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'with',\n",
       " 'python',\n",
       " 'and/or',\n",
       " 'scala',\n",
       " 'experi',\n",
       " ',',\n",
       " 'pleas',\n",
       " 'appli',\n",
       " 'today',\n",
       " '!',\n",
       " 'applic',\n",
       " 'must',\n",
       " 'be',\n",
       " 'author',\n",
       " 'to',\n",
       " 'work',\n",
       " 'in',\n",
       " 'the',\n",
       " 'u.s.',\n",
       " 'cybercod',\n",
       " ',',\n",
       " 'inc',\n",
       " 'is',\n",
       " 'proud',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'equal',\n",
       " 'opportun',\n",
       " 'employ',\n",
       " 'all',\n",
       " 'qualifi',\n",
       " 'applic',\n",
       " 'will',\n",
       " 'receiv',\n",
       " 'consider',\n",
       " 'for',\n",
       " 'employ',\n",
       " 'without',\n",
       " 'regard',\n",
       " 'to',\n",
       " 'race',\n",
       " ',',\n",
       " 'color',\n",
       " ',',\n",
       " 'religion',\n",
       " ',',\n",
       " 'sex',\n",
       " ',',\n",
       " 'nation',\n",
       " 'origin',\n",
       " ',',\n",
       " 'disabl',\n",
       " ',',\n",
       " 'protect',\n",
       " 'veteran',\n",
       " 'status',\n",
       " ',',\n",
       " 'or',\n",
       " 'ani',\n",
       " 'other',\n",
       " 'characterist',\n",
       " 'protect',\n",
       " 'by',\n",
       " 'law',\n",
       " '.',\n",
       " 'your',\n",
       " 'right',\n",
       " 'to',\n",
       " 'work',\n",
       " '–',\n",
       " 'in',\n",
       " 'complianc',\n",
       " 'with',\n",
       " 'feder',\n",
       " 'law',\n",
       " ',',\n",
       " 'all',\n",
       " 'person',\n",
       " 'hire',\n",
       " 'will',\n",
       " 'be',\n",
       " 'requir',\n",
       " 'to',\n",
       " 'verifi',\n",
       " 'ident',\n",
       " 'and',\n",
       " 'elig',\n",
       " 'to',\n",
       " 'work',\n",
       " 'in',\n",
       " 'the',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'and',\n",
       " 'to',\n",
       " 'complet',\n",
       " 'the',\n",
       " 'requir',\n",
       " 'employ',\n",
       " 'elig',\n",
       " 'verif',\n",
       " 'document',\n",
       " 'form',\n",
       " 'upon',\n",
       " 'hire',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "stemmed = []\n",
    "for word in wtkn:\n",
    "    stemmed.append(sn_stemm.stem(word))\n",
    "\n",
    "stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fully >>> fully\nremote >>> remote\n >>> \ndata >>> data\nscientist >>> scientist\ndata >>> data\nscientist >>> scientist\npython >>> python\nandor >>> andor\nscala >>> scala\nexperience >>> experience\nplease >>> please\nread >>> read\non >>> on\nwere >>> were\nbased >>> based\nnew >>> new\nyork >>> york\ncity >>> city\narea >>> area\nproven >>> proven\ngrowing >>> growing\ncommercial >>> commercial\nreal >>> real\nestate >>> estate\ntechnology >>> technology\ncompany >>> company\nwere >>> were\ninnovators >>> innovator\nresponsible >>> responsible\ncreating >>> creating\nstate >>> state\nart >>> art\ntechnology >>> technology\nhelps >>> help\nmake >>> make\ncommercial >>> commercial\nreal >>> real\nestate >>> estate\nindustry >>> industry\nefficient >>> efficient\nhighly >>> highly\nadvanced >>> advanced\nculture >>> culture\neverything >>> everything\nus >>> u\nwere >>> were\nsmall >>> small\ngroup >>> group\nlike >>> like\nminded >>> minded\npeople >>> people\nshare >>> share\npassion >>> passion\nproviding >>> providing\nhighest >>> highest\nquality >>> quality\nproduct >>> product\nlooking >>> looking\npeople >>> people\njoin >>> join\npassion >>> passion\ntechnology >>> technology\nworking >>> working\ncollaboratively >>> collaboratively\nlooking >>> looking\npart >>> part\ncompany >>> company\nchanging >>> changing\nindustry >>> industry\ninnovation >>> innovation\nfun >>> fun\nit >>> it\ndata >>> data\nscientist >>> scientist\nplay >>> play\ncritical >>> critical\nrole >>> role\ncreating >>> creating\nmodels >>> model\nembedded >>> embedded\ndata >>> data\npipelines >>> pipeline\nsupport >>> support\nentire >>> entire\nplatform >>> platform\nwork >>> work\nnext >>> next\nlevel >>> level\ndata >>> data\ninfrastructure >>> infrastructure\nsupports >>> support\nlinks >>> link\ndatasets >>> datasets\nscale >>> scale\nbuilding >>> building\nbrand >>> brand\nnew >>> new\nproduct >>> product\nwithin >>> within\ncompany >>> company\nleveraging >>> leveraging\nmachine >>> machine\nlearning >>> learning\nai >>> ai\nbetter >>> better\nserve >>> serve\ncustomers >>> customer\nenhance >>> enhance\nway >>> way\nrun >>> run\nbusiness >>> business\naddition >>> addition\ncollaborate >>> collaborate\nstellar >>> stellar\nengineering >>> engineering\nteam >>> team\nensure >>> ensure\nsupport >>> support\nprocess >>> process\nefficient >>> efficient\nget >>> get\nprojects >>> project\ndone >>> done\ntimely >>> timely\nmanner >>> manner\nwork >>> work\nteam >>> team\nintegral >>> integral\npart >>> part\nsuccess >>> success\ncompany >>> company\ntop >>> top\nreasons >>> reason\nwork >>> work\nusgreat >>> usgreat\ncompensationbenefitssteady >>> compensationbenefitssteady\nroom >>> room\ngrowthwork >>> growthwork\nhigh >>> high\nend >>> end\ninnovative >>> innovative\ntechnology >>> technology\nawesome >>> awesome\nfamily >>> family\nlike >>> like\nculture >>> culture\ndoingimprove >>> doingimprove\ndata >>> data\nquality >>> quality\nwork >>> work\nimprove >>> improve\ncustomer >>> customer\naccess >>> access\ndata >>> data\npart >>> part\nanalytical >>> analytical\nwork >>> work\nfeature >>> feature\ngeneration >>> generation\nmodel >>> model\nchoice >>> choice\nimplementation >>> implementation\nwithin >>> within\nplatformwork >>> platformwork\ninsure >>> insure\nefficiency >>> efficiency\nproduct >>> product\nwriting >>> writing\nquality >>> quality\ncode >>> code\nparticipating >>> participating\ntesting >>> testing\nreviewswrite >>> reviewswrite\nprocess >>> process\nmetrics >>> metric\nhelp >>> help\nmeasure >>> measure\nimprove >>> improve\nperformance >>> performance\nwork >>> work\nengineering >>> engineering\nteam >>> team\ndesign >>> design\nbuild >>> build\nmaintain >>> maintain\nml >>> ml\nmodels >>> model\nneed >>> need\nposition >>> position\nleast >>> least\nyears >>> year\nexperience >>> experience\nworking >>> working\nwith >>> with\npython >>> python\nandor >>> andor\nscalademonstrate >>> scalademonstrate\nstrong >>> strong\nexperience >>> experience\nfeature >>> feature\ndevelopment >>> development\nrandom >>> random\nforests >>> forest\nnice >>> nice\nhave >>> have\ndistributed >>> distributed\ncomputing >>> computing\nspark >>> spark\nandor >>> andor\npysparkworking >>> pysparkworking\nknowledge >>> knowledge\nsqlredshifthadoop >>> sqlredshifthadoop\nso >>> so\ndata >>> data\nscientist >>> scientist\npython >>> python\nandor >>> andor\nscala >>> scala\nexperience >>> experience\nplease >>> please\napply >>> apply\ntodayapplicants >>> todayapplicants\nmust >>> must\nauthorized >>> authorized\nwork >>> work\nus >>> u\ncybercoders >>> cybercoders\ninc >>> inc\nproud >>> proud\nequal >>> equal\nopportunity >>> opportunity\nemployer >>> employer\nqualified >>> qualified\napplicants >>> applicant\nreceive >>> receive\nconsideration >>> consideration\nemployment >>> employment\nwithout >>> without\nregard >>> regard\nrace >>> race\ncolor >>> color\nreligion >>> religion\nsex >>> sex\nnational >>> national\norigin >>> origin\ndisability >>> disability\nprotected >>> protected\nveteran >>> veteran\nstatus >>> status\ncharacteristic >>> characteristic\nprotected >>> protected\nlaw >>> law\nright >>> right\nwork >>> work\n >>> \ncompliance >>> compliance\nfederal >>> federal\nlaw >>> law\npersons >>> person\nhired >>> hired\nrequired >>> required\nverify >>> verify\nidentity >>> identity\neligibility >>> eligibility\nwork >>> work\nunited >>> united\nstates >>> state\ncomplete >>> complete\nrequired >>> required\nemployment >>> employment\neligibility >>> eligibility\nverification >>> verification\ndocument >>> document\nform >>> form\nupon >>> upon\nhire >>> hire\n"
     ]
    }
   ],
   "source": [
    "for word in rgx_split:\n",
    "    print(f\"{word} >>> {lemmatizer.lemmatize(word)}\")"
   ]
  }
 ]
}